{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Conversation Analysis\n",
    "\n",
    "This notebook analyzes [medical conversations](https://www.kaggle.com/datasets/artemminiailo/medicalconversations2disease) to predict diseases and highlight relevant words using Spacy and deep learning."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Download spacy model\n",
    "!python -m spacy download en_core_web_sm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load and preprocess the data\n",
    "df = pd.read_csv('data/medical_conversations.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nSample conversations:\")\n",
    "display(df.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def remove_bot_responses(text):\n",
    "    # Split the conversation into turns\n",
    "    turns = text.split('</s>')\n",
    "    # Keep only user turns (they start with \"User:\") and remove the \"User:\" prefix\n",
    "    user_turns = [turn.strip()[5:].strip() for turn in turns if turn.strip().startswith('User:')]\n",
    "    # Join the user turns back together\n",
    "    return ' </s> '.join(user_turns)\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    # First remove bot responses\n",
    "    text = remove_bot_responses(text)\n",
    "    # Remove HTML-like tags (including </s> tags)\n",
    "    import re\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    # Remove stopwords and punctuation, convert to lowercase\n",
    "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['processed_text'] = df['conversations'].apply(preprocess_text)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['disease_encoded'] = label_encoder.fit_transform(df['disease'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create vocabulary and word embeddings\n",
    "class Vocabulary:\n",
    "    def __init__(self, texts):\n",
    "        self.word2idx = {'<PAD>': 0}\n",
    "        self.idx2word = {0: '<PAD>'}\n",
    "        self.build_vocab(texts)\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        idx = 1\n",
    "        for text in texts:\n",
    "            for word in text.split():\n",
    "                if word not in self.word2idx:\n",
    "                    self.word2idx[word] = idx\n",
    "                    self.idx2word[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "# Create vocabulary\n",
    "vocab = Vocabulary(df['processed_text'])\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create dataset class\n",
    "class MedicalDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_length=100):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx].split()\n",
    "        # Convert words to indices and pad\n",
    "        indices = [self.vocab.word2idx.get(word, 0) for word in text[:self.max_length]]\n",
    "        indices = indices + [0] * (self.max_length - len(indices))\n",
    "        return torch.tensor(indices), torch.tensor(self.labels[idx])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['processed_text'], df['disease_encoded'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MedicalDataset(X_train.values, y_train.values, vocab)\n",
    "test_dataset = MedicalDataset(X_test.values, y_test.values, vocab)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define the model\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "\n",
    "        # Calculate attention weights\n",
    "        attention_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        self.last_attention_weights = attention_weights  # Store for later visualization\n",
    "\n",
    "        # Apply attention\n",
    "        attended = torch.sum(attention_weights * lstm_out, dim=1)\n",
    "        return self.fc(attended)\n",
    "\n",
    "# Initialize model\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 64\n",
    "OUTPUT_DIM = len(label_encoder.classes_)\n",
    "\n",
    "model = TextClassifier(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_texts, batch_labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_texts)\n",
    "        loss = criterion(predictions, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_texts, batch_labels in test_loader:\n",
    "            predictions = model(batch_texts)\n",
    "            _, predicted = torch.max(predictions, 1)\n",
    "            total += batch_labels.size(0)\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print(f'Average Loss: {total_loss/len(train_loader):.4f}')\n",
    "    print(f'Accuracy: {100 * correct / total:.2f}%\\n')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Function to visualize word relevance\n",
    "def visualize_word_relevance(text, attention_weights, vocab):\n",
    "    words = text.split()\n",
    "    weights = attention_weights.squeeze().numpy()[:len(words)]\n",
    "\n",
    "    # Normalize weights to [0, 1]\n",
    "    weights = (weights - weights.min()) / (weights.max() - weights.min())\n",
    "\n",
    "    # Create HTML with colored words\n",
    "    html = []\n",
    "    for word, weight in zip(words, weights):\n",
    "        color_intensity = int(255 * (1 - weight))\n",
    "        html.append(f'<span style=\"background-color: rgba(255, {color_intensity}, {color_intensity}, 0.5)\">{word}</span>')\n",
    "\n",
    "    return ' '.join(html)\n",
    "\n",
    "# Display examples with highlighted words\n",
    "model.eval()\n",
    "print(\"Examples with word relevance highlighting:\")\n",
    "\n",
    "sample_indices = np.random.choice(len(test_dataset), 10, replace=False)\n",
    "for idx in sample_indices:\n",
    "    text_indices, label = test_dataset[idx]\n",
    "    text = X_test.iloc[idx]\n",
    "\n",
    "    # Get model prediction and attention weights\n",
    "    with torch.no_grad():\n",
    "        prediction = model(text_indices.unsqueeze(0))\n",
    "        attention_weights = model.last_attention_weights\n",
    "\n",
    "    pred_label = label_encoder.inverse_transform([prediction.argmax().item()])[0]\n",
    "    true_label = label_encoder.inverse_transform([label.item()])[0]\n",
    "\n",
    "    print(f\"\\nTrue label: {true_label}\")\n",
    "    print(f\"Predicted label: {pred_label}\")\n",
    "    display(HTML(visualize_word_relevance(text, attention_weights, vocab)))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Challenge\n",
    "- Can you come up with an explanation why the attention is at those words?\n",
    "- Can you write some code to calculate the top 5 relevant words across all results?\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
